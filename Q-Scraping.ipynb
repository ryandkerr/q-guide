{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers and librares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ryan Kerr\n",
    "# q-scraper2.py\n",
    "# python web scraper for the Harvard Q guide\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import requests\n",
    "import pdb\n",
    "\n",
    "BASE_URL = \"https://webapps.fas.harvard.edu/course_evaluation_reports/fas/list?yearterm=2014_1\"\n",
    "\n",
    "# with help from Nikhil Benesch's gist:\n",
    "# https://gist.github.com/benesch/43515655b1f877779522\n",
    "# This session ID can be obtained by signing into the Q from your\n",
    "# browser and inspecting the value of the JSESSIONID cookie. It expires\n",
    "# frequently!\n",
    "SESSION_ID = 'DF974F67A332CC78D578E29785D0C550'\n",
    " \n",
    "# Q URLs\n",
    "Q_BASE = \"https://webapps.fas.harvard.edu/course_evaluation_reports/fas/\"\n",
    "Q_LIST = 'https://webapps.fas.harvard.edu/course_evaluation_reports/fas/list?yearterm=2014_1'\n",
    "\n",
    "# helper for getting page source using cookies\n",
    "def get_page_source(section_url):\n",
    "    session = requests.Session()\n",
    "    session.cookies = requests.utils.cookiejar_from_dict({\n",
    "        'JSESSIONID': SESSION_ID\n",
    "    })\n",
    "    session.verify = False\n",
    "    request = session.get(section_url)\n",
    "    return request.text\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actually scrape the pages for a given year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rdk/anaconda2/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py:791: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html\n",
      "  InsecureRequestWarning)\n",
      "/home/rdk/anaconda2/lib/python2.7/site-packages/requests/packages/urllib3/connectionpool.py:791: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.org/en/latest/security.html\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 1259\n",
      "1 / 1259"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The soution here is not elegant but it works. Go to the\n",
    "q page for a given semester and click all of the arrows\n",
    "to expand each department. Then \"save as\" the page to the\n",
    "data folder for this project. This way we can access all\n",
    "of the course links (because they need to be expanded\n",
    "manually).\n",
    "\"\"\"\n",
    "courses = []\n",
    "with open(\"data/Course Evaluations_ Course Page_spring2015.html\", \"r\") as f:\n",
    "    html = f.read()\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    courses = soup.find_all(\"li\", attrs={\"class\": \"course\"})\n",
    "course_urls = map(lambda x: x.find(\"a\")[\"href\"], courses)\n",
    "\n",
    "# create folder to store scraped pages\n",
    "semester = \"2015_spring\"\n",
    "file_dump = semester + \"_pages\"\n",
    "if not os.path.exists(file_dump):\n",
    "    os.mkdir(file_dump)\n",
    "\n",
    "# save all scraped class pages\n",
    "for i, course_url in enumerate(course_urls):\n",
    "    print str(i) + \" / \" + str(len(course_urls)) \n",
    "    time.sleep(5.0 + random.uniform(0.0, 1.0))\n",
    "    \n",
    "    # save original page with aggregate statistics\n",
    "    with open(file_dump+\"/class\"+str(i)+\"main.txt\", \"wb\") as f:\n",
    "        f.write(get_page_source(course_url).encode(\"ascii\", \"ignore\"))\n",
    "        \n",
    "    time.sleep(5.0 + random.uniform(0.0, 1.0))\n",
    "        \n",
    "    # this scrapes the page with all student reviews\n",
    "    with open(file_dump+\"/class\"+str(i)+\"reviews.txt\", \"wb\") as f:\n",
    "        f.write(get_page_source(course_url + \"&qid=1487&sect_num=\").encode(\"ascii\", \"ignore\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
